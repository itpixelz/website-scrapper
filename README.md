# ğŸŒ Website Documentation Scraper

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

A powerful Python-based documentation scraper that downloads and converts documentation websites into well-structured markdown files. Perfect for offline documentation reading, content migration, or documentation archival.

## âœ¨ Features

- ğŸ“š **Complete Documentation Scraping**: Downloads entire documentation websites
- ğŸŒ³ **Structure Preservation**: Maintains original site hierarchy
- âš¡ **Smart Version Handling**: Detects and respects documentation versions
- ğŸ”„ **Format Conversion**: Converts HTML to clean Markdown
- ğŸ›¡ï¸ **Robust Error Handling**: Rate limiting and retry logic
- ğŸ“ **Comprehensive Logging**: Detailed logs for debugging
- ğŸ§ª **Test Coverage**: Includes unit tests
- ğŸ¯ **Framework Support**: Works with Sphinx, Docusaurus, and more

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- pip (Python package installer)

### Installation

1. Clone this repository:
```bash
git clone https://github.com/yourusername/website-scraper.git
cd website-scraper
```

2. Set up a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

### Basic Usage

```bash
# Basic scraping
python website-scraper.py https://example.com/docs

# Custom output directory
python website-scraper.py https://example.com/docs --output-dir custom_docs

# Adjust retry attempts
python website-scraper.py https://example.com/docs --max-retries 5
```

## ğŸ“ Project Structure

```
website-scraper/
â”œâ”€â”€ ğŸ“‚ docs/                 # Scraped documentation storage
â”‚   â”œâ”€â”€ example.com/        # Domain-specific content
â”‚   â””â”€â”€ other-site.com/     # Separate by domain
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                # Log files directory
â”‚   â”œâ”€â”€ example.com_scraping.log
â”‚   â””â”€â”€ other-site.com_scraping.log
â”‚
â”œâ”€â”€ ğŸ“„ website-scraper.py   # Main script
â”œâ”€â”€ ğŸ“„ requirements.txt     # Dependencies
â”œâ”€â”€ ğŸ“„ README.md           # Documentation
â””â”€â”€ ğŸ“‚ tests/              # Test files
```

## ğŸ”§ Configuration

The scraper includes smart defaults and can be configured through command-line arguments:

```bash
Options:
  --output-dir DIR    Custom output directory (default: docs/<domain>)
  --max-retries N     Maximum retry attempts for failed requests (default: 3)
```

### Automatic Features:
- ğŸ” Version detection in URLs
- ğŸš« Asset filtering (images, CSS, JS)
- â±ï¸ Rate limiting
- ğŸ”„ Retry logic for failed requests
- ğŸ§¹ Clean Markdown conversion

## ğŸ“ Output Format

### Documentation Files
- Stored in `docs/<domain>/`
- Maintains original site structure
- Converts to clean Markdown format
- Preserves metadata and links

### Logs
- Stored in `logs/<domain>_scraping.log`
- Includes timestamps and error details
- Tracks scraping progress
- Helps with debugging

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. Fork the repository
2. Create your feature branch:
```bash
git checkout -b feature/amazing-feature
```
3. Commit your changes:
```bash
git commit -m 'Add amazing feature'
```
4. Push to the branch:
```bash
git push origin feature/amazing-feature
```
5. Open a Pull Request

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Beautiful Soup for HTML parsing
- Requests for HTTP handling
- All our contributors and users

## ğŸ“§ Support

- Create an issue for bug reports
- Start a discussion for feature requests
- Check existing issues before posting

---
Made with â¤ï¸ by [Your Name] 